A Retrieval-Augmented Generation (RAG) based application that allows users to upload a PDF and ask questions, receiving accurate answers grounded in the document.

This project combines LangChain, ChromaDB, Groq LLM, and Streamlit to create a lightweight yet powerful Question-Answering system.

ðŸš€ Features

âœ… Upload any PDF document
âœ… Automatic text extraction, chunking & embedding
âœ… Vector database powered retrieval using ChromaDB
âœ… Precise answer generation using Groq LLM models
âœ… Clean & interactive Streamlit UI
âœ… Reduces hallucinations by retrieving exact context from documents
rag_doc_assistant/
â”‚
â”œâ”€â”€ app.py                  # Main Streamlit application
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ README.md               # Documentation
â”œâ”€â”€ .gitignore              # Ignore vector DB cache
â””â”€â”€ data/
    â””â”€â”€ sample_docs

requirment


cd rag_doc_assistant
pip install -r requirements.txt
set GROQ_API_KEY=your_key_here
streamlit run app.py


    ðŸ’¡ How It Works

User uploads a PDF

PDF is split into text chunks

Text chunks are embedded into vectors

ChromaDB stores and retrieves most relevant chunks

Groq LLM generates answers grounded in retrieved info

User sees responses instantly
